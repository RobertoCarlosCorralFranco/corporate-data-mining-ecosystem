# Corporate Data Extraction and Classification System

## Ecosystem Overview
This Python software ecosystem represents a comprehensive solution for Web Scraping and Unstructured Data Mining, specifically designed to feed Large Language Models (LLMs) and generate a structured database of corporate profiles.

## 1. Extraction Architecture (Data Acquisition)
The system employs a dual extraction strategy to maximize compatibility with differing web architectures:

* **Static Extraction (`URL.py`):** Utilizes the `requests` library for synchronous requests. It implements a User-Agent emulation mechanism in HTTP headers to mitigate Web Application Firewall (WAF) blocking and avoid 403 Forbidden errors.
* **Dynamic Extraction & JS Rendering (`extractor.py`, `procesador.py`):** Implements Selenium WebDriver with `ChromeDriverManager`. This layer is critical for modern websites (Single Page Applications) that rely on Client-Side Rendering. The script includes a programmed latency (`time.sleep(5)`) to ensure full DOM hydration prior to `page_source` capture.

## 2. Processing and Sanitization (Data Sanitization)
For the transformation of raw HTML into AI-readable plain text:

* **Parsing:** Utilizes `BeautifulSoup4` with the `html.parser` engine.
* **Text Normalization:** The system implements cleaning logic that strips markup tags, scripts, and styles, filtering empty lines via list comprehensions to reduce prompt noise (token efficiency).
* **Dynamic Nomenclature:** The `extractor.py` script includes a URL parsing module (`urllib.parse`) to generate filenames based on the domain (`netloc`), ensuring data traceability.

## 3. Persistence and Database Management (`guardar_categoria.py`)
The storage architecture is based on the CSV (Comma-Separated Values) standard to facilitate interoperability with data analysis tools:

* **Data Schema:** Defined by three fundamental axes: `Company_Name`, `Primary_Industry` (based on standard taxonomies like NAICS/GICS), and `Talent_Verticals`.
* **Automatic DB Management:** The system verifies the existence of the physical file and, if absent, initializes the structure with corresponding headers via the `w` (write) open mode.
* **Data Integrity:** Utilizes the `a` (append) mode for writing new rows, preventing historical data loss and handling `utf-8` encoding to support multilingual characters.

## 4. Workflow (Human-AI Handoff)
The `procesador_automatico.py` script acts as the Central Orchestrator, establishing a hybrid workflow:

1.  **Automation:** Captures and sanitizes content from the provided URL.
2.  **Handoff:** Presents clean text for analysis by an AI agent (Gemini).
3.  **Human Feedback:** Allows for manual entry of the categorization generated by the AI.
4.  **Loop Closure:** Stores the final result in the persistent database.

## Dependencies
To execute this ecosystem, install the necessary dependencies:

```bash
pip install -r requirements.txt
